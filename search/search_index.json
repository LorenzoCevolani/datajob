{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs \u00b6 For full documentation visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"api/","text":"API documentation \u00b6 Datajob CLI \u00b6 deploy Source code in datajob/datajob.py @app . command ( context_settings = { \"allow_extra_args\" : True , \"ignore_unknown_options\" : True } ) def deploy ( stage : str = typer . Option ( None ), config : str = typer . Option ( Path , callback = os . path . abspath ), package : str = typer . Option ( None , \"--package\" ), ctx : typer . Context = typer . Option ( list ), ): if package : # todo - check if we are building in the right directory project_root = str ( Path ( config ) . parent ) wheel . create_wheel ( project_root = project_root , package = package ) # create stepfunctions if requested # make sure you have quotes around the app arguments args = [ \"--app\" , f \"\"\" \"python { config } \" \"\"\" , \"-c\" , f \"stage= { stage } \" ] extra_args = ctx . args call_cdk ( command = \"deploy\" , args = args , extra_args = extra_args ) synthesize Source code in datajob/datajob.py @app . command ( context_settings = { \"allow_extra_args\" : True , \"ignore_unknown_options\" : True } ) def synthesize ( stage : str = typer . Option ( None ), config : str = typer . Option ( Path , callback = os . path . abspath ), ctx : typer . Context = typer . Option ( list ), ): args = [ \"--app\" , f \"\"\" \"python { config } \" \"\"\" , \"-c\" , f \"stage= { stage } \" ] extra_args = ctx . args call_cdk ( command = \"synthesize\" , args = args , extra_args = extra_args ) destroy Source code in datajob/datajob.py @app . command ( context_settings = { \"allow_extra_args\" : True , \"ignore_unknown_options\" : True } ) def destroy ( stage : str = typer . Option ( None ), config : str = typer . Option ( Path , callback = os . path . abspath ), ctx : typer . Context = typer . Option ( list ), ): args = [ \"--app\" , f \"\"\" \"python { config } \" \"\"\" , \"-c\" , f \"stage= { stage } \" ] extra_args = ctx . args call_cdk ( command = \"destroy\" , args = args , extra_args = extra_args ) DatajobStack \u00b6 __enter__ ( self ) special As soon as we enter the contextmanager, we create the datajob context. Returns: Type Description datajob stack. Source code in datajob/datajob_stack.py def __enter__ ( self ): \"\"\" As soon as we enter the contextmanager, we create the datajob context. :return: datajob stack. \"\"\" self . context = DatajobContext ( self , unique_stack_name = self . unique_stack_name , project_root = self . project_root , include_folder = self . include_folder , ) return self __exit__ ( self , exc_type , exc_value , traceback ) special steps we have to do when exiting the context manager. - we will create the resources we have defined. - we will synthesize our stack so that we have everything to deploy. Parameters: Name Type Description Default exc_type required exc_value required traceback required Returns: Type Description None Source code in datajob/datajob_stack.py def __exit__ ( self , exc_type , exc_value , traceback ): \"\"\" steps we have to do when exiting the context manager. - we will create the resources we have defined. - we will synthesize our stack so that we have everything to deploy. :param exc_type: :param exc_value: :param traceback: :return: None \"\"\" logger . debug ( \"creating resources and synthesizing stack.\" ) self . create_resources () self . scope . synth () __init__ ( self , stack_name , stage = None , project_root = None , include_folder = None , account = None , region = None , scope =< aws_cdk . core . App object at 0x7f741ec0e0a0 > , ** kwargs ) special Parameters: Name Type Description Default scope Construct aws cdk core construct object. <aws_cdk.core.App object at 0x7f741ec0e0a0> stack_name str a name for this stack. required stage str the stage name to which we are deploying None project_root str the path to the root of this project None include_folder str specify the name of the folder we would like to include in the deployment bucket. None account str AWS account number None region str AWS region where we want to deploy our datajob to None kwargs any extra kwargs for the core.Construct {} Source code in datajob/datajob_stack.py def __init__ ( self , stack_name : str , stage : str = None , project_root : str = None , include_folder : str = None , account : str = None , region : str = None , scope : core . Construct = core . App (), ** kwargs , ) -> None : \"\"\" :param scope: aws cdk core construct object. :param stack_name: a name for this stack. :param stage: the stage name to which we are deploying :param project_root: the path to the root of this project :param include_folder: specify the name of the folder we would like to include in the deployment bucket. :param account: AWS account number :param region: AWS region where we want to deploy our datajob to :param kwargs: any extra kwargs for the core.Construct \"\"\" account = ( account if account is not None else os . environ . get ( \"AWS_DEFAULT_ACCOUNT\" ) ) region = region if region is not None else os . environ . get ( \"AWS_DEFAULT_REGION\" ) env = { \"region\" : region , \"account\" : account } self . scope = scope self . stage = self . get_stage ( stage ) self . unique_stack_name = self . _create_unique_stack_name ( stack_name , self . stage ) super () . __init__ ( scope = scope , id = self . unique_stack_name , env = env , ** kwargs ) self . project_root = project_root self . include_folder = include_folder self . resources = [] self . context = None create_resources ( self ) create each of the resources of this stack Source code in datajob/datajob_stack.py def create_resources ( self ): \"\"\"create each of the resources of this stack\"\"\" [ resource . create () for resource in self . resources ] get_context_parameter ( self , name ) get a cdk context parameter from the cli. Source code in datajob/datajob_stack.py def get_context_parameter ( self , name : str ) -> str : \"\"\"get a cdk context parameter from the cli.\"\"\" context_parameter = self . scope . node . try_get_context ( name ) if not context_parameter : raise ValueError ( \"we expect a stage to be set on the cli. e.g 'cdk deploy -c stage=my-stage'\" ) logger . debug ( f \"context parameter { name } found.\" ) return context_parameter get_stage ( self , stage ) get the stage parameter and return a default if not found. Source code in datajob/datajob_stack.py def get_stage ( self , stage ): \"\"\"get the stage parameter and return a default if not found.\"\"\" try : if stage == \"None\" or not stage : logger . debug ( \"No stage is passed to datajob stack, taking the default one.\" ) return DataJobStack . DEFAULT_STAGE elif stage : logger . debug ( \"a stage parameter is passed via the cli or via the datajob stack configuration file.\" ) return stage except ValueError : return DataJobStack . DEFAULT_STAGE DatajobContext \u00b6 DatajobContext is a class that creates all the services necessary for a datajob to run. You have to instantiate this class once per DatajobStack. __init__ ( self , scope , unique_stack_name , project_root = None , include_folder = None , ** kwargs ) special Parameters: Name Type Description Default scope Construct aws cdk core construct object. required unique_stack_name str a unique name for this stack. like this the name of our resources will not collide with other deployments. required stage the stage name to which we are deploying required project_root str the path to the root of this project None include_folder str specify the name of the folder we would like to include in the deployment bucket. None kwargs any extra kwargs for the core.Construct {} Source code in datajob/datajob_context.py def __init__ ( self , scope : core . Construct , unique_stack_name : str , project_root : str = None , include_folder : str = None , ** kwargs , ) -> None : \"\"\" :param scope: aws cdk core construct object. :param unique_stack_name: a unique name for this stack. like this the name of our resources will not collide with other deployments. :param stage: the stage name to which we are deploying :param project_root: the path to the root of this project :param include_folder: specify the name of the folder we would like to include in the deployment bucket. :param kwargs: any extra kwargs for the core.Construct \"\"\" logger . info ( \"creating datajob context.\" ) super () . __init__ ( scope , unique_stack_name , ** kwargs ) self . project_root = project_root self . unique_stack_name = unique_stack_name ( self . deployment_bucket , self . deployment_bucket_name , ) = self . _create_deployment_bucket ( self . unique_stack_name ) ( self . data_bucket , self . data_bucket_name ) = self . _create_data_bucket ( self . unique_stack_name ) self . s3_url_wheel = None if self . project_root : self . s3_url_wheel = self . _build_and_deploy_wheel ( self . unique_stack_name , self . project_root , self . deployment_bucket , self . deployment_bucket_name , ) if include_folder : self . _deploy_local_folder ( include_folder ) logger . info ( \"datajob context created.\" ) GlueJob \u00b6 __init__ ( self , datajob_stack , name , job_path , job_type = 'pythonshell' , glue_version = None , max_capacity = None , arguments = None , python_version = '3' , role = None , * args , ** kwargs ) special Parameters: Name Type Description Default datajob_stack Construct aws cdk core construct object. required name str a name for this glue job (will appear on the glue console). required job_path str the path to the glue job relative to the project root. required job_type str choose pythonshell for plain python / glueetl for a spark cluster. pythonshell is the default. 'pythonshell' glue_version str at the time of writing choose 1.0 for pythonshell / 2.0 for spark. None max_capacity int max nodes we want to run. None arguments dict the arguments as a dict for this glue job. None python_version str 3 is the default '3' role Role you can provide a cdk iam role object as arg. if not provided this class will instantiate a role, None args any extra args for the glue.CfnJob () kwargs any extra kwargs for the glue.CfnJob {} Source code in datajob/glue/glue_job.py def __init__ ( self , datajob_stack : core . Construct , name : str , job_path : str , job_type : str = GlueJobType . PYTHONSHELL . value , glue_version : str = None , max_capacity : int = None , arguments : dict = None , python_version : str = \"3\" , role : iam . Role = None , * args , ** kwargs , ): \"\"\" :param datajob_stack: aws cdk core construct object. :param name: a name for this glue job (will appear on the glue console). :param job_path: the path to the glue job relative to the project root. :param job_type: choose pythonshell for plain python / glueetl for a spark cluster. pythonshell is the default. :param glue_version: at the time of writing choose 1.0 for pythonshell / 2.0 for spark. :param max_capacity: max nodes we want to run. :param arguments: the arguments as a dict for this glue job. :param python_version: 3 is the default :param role: you can provide a cdk iam role object as arg. if not provided this class will instantiate a role, :param args: any extra args for the glue.CfnJob :param kwargs: any extra kwargs for the glue.CfnJob \"\"\" logger . info ( f \"creating glue job { name } \" ) super () . __init__ ( datajob_stack , name , ** kwargs ) self . job_path = GlueJob . _get_job_path ( self . project_root , job_path ) self . arguments = arguments if arguments else {} self . job_type = GlueJob . _get_job_type ( job_type = job_type ) self . python_version = python_version self . glue_version = GlueJob . _get_glue_version ( glue_version = glue_version , job_type = job_type ) self . max_capacity = max_capacity self . role = self . _get_role ( role , self . unique_name ) self . args = args self . kwargs = kwargs logger . info ( f \"glue job { name } created.\" ) __rrshift__ ( other , self , * args , ** kwargs ) special Called for [task1, task2] >> task3 because list don't have rshift operators. Therefore we reverse the order of the arguments and call rshift Source code in datajob/glue/glue_job.py def __rrshift__ ( other , self , * args , ** kwargs ): \"\"\"Called for [task1, task2] >> task3 because list don't have __rshift__ operators. Therefore we reverse the order of the arguments and call __rshift__\"\"\" __rshift__ ( self = self , other = other ) __rshift__ ( self , other , * args , ** kwargs ) special called when doing - task1 >> task2 - task1 >> [task2,task3] Source code in datajob/glue/glue_job.py def __rshift__ ( self , other , * args , ** kwargs ): \"\"\"called when doing - task1 >> task2 - task1 >> [task2,task3] \"\"\" _handle_first ( self = self ) _connect ( other ) return self create ( self ) create datajob Source code in datajob/glue/glue_job.py def create ( self ): s3_url_glue_job = self . _deploy_glue_job_code ( context = self . context , glue_job_name = self . unique_name , path_to_glue_job = self . job_path , ) self . _create_glue_job ( context = self . context , glue_job_name = self . unique_name , s3_url_glue_job = s3_url_glue_job , arguments = self . arguments , job_type = self . job_type , python_version = self . python_version , glue_version = self . glue_version , max_capacity = self . max_capacity , * self . args , ** self . kwargs , )","title":"API documentation"},{"location":"api/#api-documentation","text":"","title":"API documentation"},{"location":"api/#datajob-cli","text":"","title":"Datajob CLI"},{"location":"api/#datajobstack","text":"","title":"DatajobStack"},{"location":"api/#datajobcontext","text":"","title":"DatajobContext"},{"location":"api/#gluejob","text":"","title":"GlueJob"}]}